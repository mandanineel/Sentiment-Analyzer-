{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dhruvil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer , AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img_path = 'tweet05.jpg'\n",
    "img = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if img is None:\n",
    "    print(f'Failed to load image from file: {img_path}')\n",
    "else:\n",
    "    # Pre-processing\n",
    "    def preprocess_image(img):\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Threshold the image\n",
    "        _, thresh = cv2.threshold(gray, 210, 230, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Dilate the image\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "        dilated = cv2.dilate(thresh, kernel, iterations=2)\n",
    "        \n",
    "        # Erode the image\n",
    "        eroded = cv2.erode(dilated, kernel, iterations=2)\n",
    "        \n",
    "        #  Gaussian blur\n",
    "        blurred = cv2.GaussianBlur(eroded, (1, 1), 0)\n",
    "        \n",
    "        return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli @\n",
      "@imVkohli\n",
      "\n",
      "Numero Uno showing why he's the best in the world.\n",
      "\n",
      "Didn't watch it live but I'm sure this was another video\n",
      "game innings by him. 4 @surya_14kumar\n",
      "\n",
      "2:35 PM - Nov 20, 2022 - Twitter for Android\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not isinstance(img, np.ndarray):\n",
    "        print(f'img is not a valid numpy array: {img}')\n",
    "else:\n",
    "        # Apply pre-processing to image\n",
    "        preprocessed_img = preprocess_image(img)\n",
    "\n",
    "        # Run OCR on pre-processed image\n",
    "        ocr_text = pytesseract.image_to_string(preprocessed_img)\n",
    "        print(ocr_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ocr_text + 'ðŸ˜Š'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virat Kohli @ @user Numero Uno showing why he's the best in the world. Didn't watch it live but I'm sure this was another video game innings by him. 4 @user 2:35 PM - Nov 20, 2022 - Twitter for Android ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "tweet_words = []\n",
    "for word in tweet.split():\n",
    "    word = word.strip()  # Remove leading/trailing whitespace\n",
    "    if word.startswith('@') and len(word) > 1:\n",
    "        word = '@user'\n",
    "    elif word.startswith('http'):\n",
    "        word = \"http\"\n",
    "    tweet_words.append(word)\n",
    "\n",
    "# Create a new list with '\\n' removed from each word\n",
    "tweet_words_no_newline = [word.replace('\\n', '') for word in tweet_words]\n",
    "revised_tweet = \" \".join(tweet_words_no_newline)\n",
    "print(revised_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "roberta = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative 0.0044868956\n",
      "Neutral 0.08884943\n",
      "Positive 0.90666366\n"
     ]
    }
   ],
   "source": [
    "# sentiment analysis\n",
    "encoded_tweet = tokenizer(revised_tweet, return_tensors='pt')\n",
    "# output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
    "output = model(**encoded_tweet)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 0.607390284538269\n",
      "Neutral: 0.11966809630393982\n",
      "Positive: 0.09580015391111374\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "    # Tokenize text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    # Join tokens back into text\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "# Preprocess text\n",
    "preprocessed_text = preprocess_text(revised_tweet)\n",
    "\n",
    "# Tokenize text and convert to input IDs and attention mask\n",
    "inputs = tokenizer(preprocessed_text, padding=True, truncation=True, return_tensors='pt')\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask)\n",
    "    scores = outputs[0]\n",
    "    probs = softmax(scores, axis=1).tolist()[0]\n",
    "\n",
    "# Print results\n",
    "labels = ['Negative', 'Neutral', 'Positive']\n",
    "for label, prob in zip(labels, probs):\n",
    "    print(f'{label}: {prob}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
